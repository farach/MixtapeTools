# Personas

> **What's in this folder:** Systematic audit and replication protocols. These are not vague "be critical" instructions — they are checklists with specific tasks, deliverables, formal referee reports, and a revise & resubmit process.

---

## Core Concept: The Health Inspector Model

Think of these personas not as "moods" for Claude, but as **health inspectors arriving at a restaurant**:

1. They have a **checklist** of specific things to verify
2. They **perform replication** (not just read code — rewrite it in other languages, compare outputs)
3. They **file two deliverables**:
   - **Referee report (markdown)** — detailed findings for the author's Claude to process during revisions
   - **Referee report deck (PDF)** — visual presentation for the human author to review, like an editor's letter summarizing the referee's concerns
4. They trigger a **revise & resubmit process** with documented correspondence

The markdown and deck serve different audiences: the markdown is machine-readable documentation that the author's Claude can reference when making fixes; the deck is human-readable communication that helps the author quickly understand what needs attention.

The key insight: **you cannot grade your own homework.** If you ask the same Claude instance that wrote code to review that code, you're asking a student to grade their own exam. The "review" will rationalize existing choices rather than challenge them.

---

## Contents

### `referee2.md` — The Systematic Audit & Replication Protocol

**What it is:** A comprehensive audit and replication protocol that performs five distinct audits, creates cross-language replication scripts, and produces a formal referee report.

**The Five Audits:**

| Audit | Purpose | Key Action |
|-------|---------|------------|
| **1. Code Audit** | Find coding errors, missing value problems, logic gaps | Scrutinize cleaning code, merge diagnostics, variable construction |
| **2. Cross-Language Replication** | Exploit orthogonality of hallucination errors | Create `referee2_replicate_*.do`, `*.R`, `*.py` scripts; compare to 6 decimal places |
| **3. Directory Audit** | Ensure replication-package readiness | Check folder structure, relative paths, naming conventions |
| **4. Output Automation Audit** | Verify programmatic output generation | Are tables/figures generated by code or manually created? |
| **5. Econometrics Audit** | Verify specification coherence | Check identification, standard errors, fixed effects, sample definition |

**Why Cross-Language Replication?**

Hallucination errors are likely **orthogonal** across LLM-produced code in different languages. If Claude writes R code with a subtle bug, asking it to write Stata code will likely produce a *different* bug. By requiring results to match across R, Stata, and Python to 6+ decimal places, you catch:

- Coding errors in any single implementation
- Package bugs you wouldn't otherwise notice
- Hallucinated syntax that happens to run without error
- Missing value handling differences
- Standard error specification differences

**Critical Rule:** Referee 2 **NEVER modifies author code**. It only reads, runs, and creates its own replication scripts. Only the author modifies the author's code.

**Deliverables:**
1. Replication scripts saved to `code/replication/` (permanent artifacts)
2. Formal referee report (markdown) at `correspondence/referee2/YYYY-MM-DD_roundN_report.md`
3. Referee report deck (Beamer PDF) at `correspondence/referee2/YYYY-MM-DD_roundN_deck.pdf` — visualizes the quantitative findings with beautiful tables and figures, following the rhetoric of decks principles (MB/MC equivalence, titles as assertions, no compilation warnings)

---

## The Revise & Resubmit Workflow

```
┌────────────────────────────────────────────────────────────┐
│  ROUND 1: INITIAL SUBMISSION                                │
├────────────────────────────────────────────────────────────┤
│  1. Author completes analysis in main Claude session        │
│  2. Author opens NEW terminal (fresh Claude)                │
│  3. Paste referee2.md, point Claude at project              │
│  4. Referee 2 performs 5 audits + creates replication       │
│     scripts in code/replication/                            │
│  5. Referee 2 files report at correspondence/referee2/      │
│  6. Close terminal                                          │
└──────────────────────────┬─────────────────────────────────┘
                           │
                           ▼
┌────────────────────────────────────────────────────────────┐
│  AUTHOR RESPONSE                                            │
├────────────────────────────────────────────────────────────┤
│  1. Author reads referee report                             │
│  2. For each Major Concern: FIX or JUSTIFY                  │
│  3. For each Minor Concern: FIX or ACKNOWLEDGE              │
│  4. Answer all Questions for Authors                        │
│  5. File response at correspondence/referee2/               │
│  6. Make code changes                                       │
└──────────────────────────┬─────────────────────────────────┘
                           │
                           ▼
┌────────────────────────────────────────────────────────────┐
│  ROUND 2+: REVISION REVIEW                                  │
├────────────────────────────────────────────────────────────┤
│  1. Open NEW terminal (fresh Claude)                        │
│  2. Paste referee2.md                                       │
│  3. Point Claude at: original report + response + code      │
│  4. Referee 2 re-runs audits, assesses responses            │
│  5. Files Round N report                                    │
│  6. Repeat until Accept or Reject                           │
└────────────────────────────────────────────────────────────┘
```

**Key points:**
- Each round requires a **new terminal** (fresh context, no prior commitments)
- Referee reports and responses go to `correspondence/referee2/`, NOT to `CLAUDE.md`
- Author must respond to every concern (fix or justify)
- Replication scripts are permanent artifacts in `code/replication/`
- Process continues until verdict is Accept or Reject

---

## Directory Structure

Your project should include:

```
your_project/
├── CLAUDE.md                         # Project context (status only, not reports)
├── correspondence/
│   └── referee2/
│       ├── 2026-02-01_round1_report.md    # Detailed written report
│       ├── 2026-02-01_round1_deck.tex     # Visual presentation
│       ├── 2026-02-01_round1_deck.pdf     # Compiled deck
│       ├── 2026-02-02_round1_response.md  # Author's response
│       ├── 2026-02-05_round2_report.md
│       ├── 2026-02-05_round2_deck.pdf
│       └── ...
├── code/
│   ├── 01_clean.R                    # Author's code (ONLY author modifies)
│   ├── 02_merge.R
│   ├── 03_estimate.R
│   └── replication/                   # Referee 2's replication scripts
│       ├── referee2_replicate_main_results.do
│       ├── referee2_replicate_main_results.R
│       ├── referee2_replicate_main_results.py
│       ├── referee2_replicate_event_study.do
│       └── ...
├── data/
│   ├── raw/
│   └── clean/
└── output/
    ├── tables/
    └── figures/
```

**Note:** Referee 2 ONLY writes to `code/replication/` and `correspondence/referee2/`. It NEVER modifies author code.

---

## How to Invoke Referee 2

### Round 1 (Initial Submission)

```
[Open new terminal]
[Paste entire contents of referee2.md]

"Please audit and replicate the project at [path].
The primary analysis language is [R/Stata/Python].
The main estimation is in [file].
Focus especially on [any specific concerns]."
```

### Round 2+ (Revision Review)

```
[Open new terminal]
[Paste entire contents of referee2.md]

"This is Round 2 of the revise & resubmit process. Please read:
1. The Round 1 referee report at correspondence/referee2/YYYY-MM-DD_round1_report.md
2. My response at correspondence/referee2/YYYY-MM-DD_round1_response.md
3. The revised code

Re-run the audits and assess whether concerns were adequately addressed."
```

---

## Philosophy

The personas in this folder exist because:

1. **AI makes confident mistakes** — Subtle bugs that compile, pass basic checks, but produce wrong results
2. **Cross-language replication catches what single-language review misses** — Hallucination errors are orthogonal *across* languages, so independent implementations expose errors that same-language review would miss
3. **Creators can't see their own blind spots** — You rationalize your own choices
4. **Academic peer review works** — The revise & resubmit process catches real problems, and as a metaphor helps the researcher better understand the function of the persona and the tasks being performed
5. **Formal process > informal vibes** — Checklists beat intuition
6. **Replication scripts are permanent artifacts** — They prove independent verification occurred and leave a fossilized record of progress that all future Claude sessions can review

---

## Future Personas

Consider creating additional audit protocols for:

- **The Statistician** — Deep dive on inference, multiple testing, power
- **The Security Reviewer** — Data privacy, credential exposure, attack vectors
- **The Pedagogy Reviewer** — For teaching materials: clarity, accuracy, accessibility

Each should follow the health inspector model: specific checklists, replication tasks, formal reports, revise & resubmit process.
